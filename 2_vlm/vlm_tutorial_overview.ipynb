{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/M2Lschool/tutorials2025-dev/blob/master/2_vlm/vlm_tutorial_overview.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# M2LS 2025: Vision-Language Models Tutorial\n",
        "---\n",
        "- Alexandre Galashov (agalashov@google.com)\n",
        "- Petra Bevandic (Petra.Bevandic@fer.hr )\n",
        "<br>\n",
        "[link to colab]\n",
        "\n",
        "**Abstract:** In this tutorial we'll explore how we can use image-text data to build **Vision Language Models** (VLMs) üöÄ. We'll start with an introduction to multimodal understanding that describes the main components of a Vision Lanugage Model. Then, we'll dive into Vision Transformer (ViT), a popular architecture which is used in VLMs as Image Encoder (Practical 1). It will be followed in Practical 2 which will dive into Contrastive Language-Image Pre-training (CLIP), a model for learning general representation from image-text pairs that can be used for a wide range of downstream tasks. We will also dive into different applications of CLIP. Finally, in Practical 3, we will actually use a pretrained VLM and do a finetuning to a given task.\n",
        "\n",
        "\n",
        "**Tutorial outline:**\n",
        "- Theory recap\n",
        "- Practical 1. ViT\n",
        "- Practical 2. CLIP in different applications -- zero-shot image classification and anomaly detection.\n",
        "- Practical 3. Using pre-trained VLM for a practical task and finetune it.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "WO5dkV8l0uDM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Theory overview\n",
        "---"
      ],
      "metadata": {
        "id": "rO-FNfGz1G2w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Vision language models (VLMs)\n",
        "\n",
        "**Vision Language Models** (VLMs) are a class of models designed to jointly process and reason about visual and textual information. They are pre-trained on vast quantities of data containing sequences of images and text, typically with the objective of predicting subsequent text given the preceding multimodal context. This process enables them to learn rich, grounded representations connecting language to visual concepts. The pretraining task for VLMs is **Visual Question Answering** (VQA), where where given (potentially multiple) images and a question (text), one needs to provide an answer (subsequent text)."
      ],
      "metadata": {
        "id": "inoqWIc21HdI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Visual Question Answering**. A key insight is that **Visual Question Answering** (VQA) can be viewed as a powerful, general-purpose interface for a wide range of vision tasks. By carefully crafting the input question, many distinct problems can be solved using the same underlying VLM architecture.\n",
        "\n",
        "Consider the following examples:\n",
        "\n",
        "**Standard VQA**: The model answers a specific, open-ended question about the image content.\n",
        "\n",
        "* Q: \"What color is the grass?\" ‚Üí A: \"Green\"\n",
        "\n",
        "**Special case VQA: Object Recognition/Classification**: The model identifies the primary subject when given a fixed, categorical question.\n",
        "\n",
        "* Q: \"What is the main object in this image?\" ‚Üí A: \"Cat\"\n",
        "\n",
        "**Special case VQA: Object Detection (as text)**: The model can even be trained to output bounding box coordinates for a queried object.\n",
        "\n",
        "* Q: \"Where is the cat?\" ‚Üí A: \"[120, 80, 450, 320]\"\n",
        "\n",
        "**Image-Text Retrieval**: The model acts as a scorer to judge the alignment between an image and a potential description. This fundamental ability powers retrieval systems. By scoring a single image against a large database of candidate texts (or vice versa), the VLM can effectively retrieve the best matches.\n",
        "\n",
        "* Q: \"What is the probability of 'A black cat on the grass' onj this image?\" ‚Üí A: 0.99\n",
        "\n",
        "**Image Captioning**: The model generates a full description when prompted with an empty or null question.\n",
        "\n",
        "* Q: \"\" ‚Üí A: \"A black cat sitting on a patch of green grass.\"\n",
        "\n",
        "This demonstrates that VQA is not just a task, but a flexible methodology for interacting with and extracting information from images.\n",
        "\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1Rsow69Td2EYz7CAJX7wyi_kLAqJN_-5s\">"
      ],
      "metadata": {
        "id": "Na4ROnsv1L6F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Main parts of a **Vision and Language model** (VLM):\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "*   Image feature extractor (Image encoder)\n",
        "*   Language feature extractor (Text encoder)\n",
        "*   Multimodal fusion\n",
        "*   Prediction head\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1vlXbjRmTEirLRpoNMm9iJFhiwwyD-clL\">"
      ],
      "metadata": {
        "id": "ZGcOktrf1Tl0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Different types of multimodal fusion\n",
        "\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=10icJo9IHPJI8jAI8QRkWoJgLtXoYX8dI\">"
      ],
      "metadata": {
        "id": "azjviEMo1U3t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pros and Cons of Fusion Strategies in Vision-Language Models:\n",
        "\n",
        "**Dual Encoder (Late Fusion):**\n",
        "\n",
        "**Pros:**\n",
        "* **Fast and efficient:** Separate feature extraction pipelines for image and text allow for independent computation and storage, enabling quick retrieval, especially with large datasets.\n",
        "* **Scalable:** Suitable for tasks like Image-Text Retrieval where comparing numerous image-text pairs is necessary.\n",
        "\n",
        "**Cons:**\n",
        "* **Less accurate:** Limited interaction between modalities only occurs at the final stage, potentially hindering performance in tasks demanding deeper understanding.\n",
        "\n",
        "**Joint Encoder (Early Fusion):**\n",
        "\n",
        "**Pros:**\n",
        "* **More accurate:**  Deep interaction between modalities through multiple computational layers (e.g., shared transformer) allows for richer understanding and better performance in complex tasks.\n",
        "* **Suitable for intricate tasks:**  Well-suited for tasks requiring complex reasoning like VQA and object detection.\n",
        "\n",
        "**Cons:**\n",
        "* **Slower and resource-intensive:** Joint encoding of image and text demands significant computational resources and time.\n",
        "* **More sensitive to data noise:**  Noisy data sources affect training more in the joint encoder setting.\n",
        "\n",
        "\n",
        "**Other Approaches:**\n",
        "\n",
        "* **Hybrid methods:** Combine aspects of both dual and cross encoders for improved accuracy and efficiency. Examples include:\n",
        "    * Retrieval using a dual encoder followed by joint-encoder re-ranking *.\n",
        "    * Dual encoder enhanced with cross-attention modules (eg. [LXMERT](https://arxiv.org/abs/1908.07490))\n",
        "    * Joint-encoder with only specific layers fused (eg. [FIBER](https://proceedings.neurips.cc/paper_files/paper/2022/file/d4b6ccf3acd6ccbc1093e093df345ba2-Paper-Conference.pdf)).\n",
        "\n",
        "\n",
        "\n",
        "*Some works like [Retrieve fast rerank smart](https://arxiv.org/abs/2103.11920) and [Thinking Fast and Slow](https://arxiv.org/abs/2103.16553) first apply the same model as a dual encoder to capitalize on the speed aspect of dual encoders, and then apply it as a joint encoder to get the most of its fine-grained understanding capabilities.\n"
      ],
      "metadata": {
        "id": "hcVvoGSs1YCt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Most popular fusion mechanism at the moment is **Early Fusion**."
      ],
      "metadata": {
        "id": "9GHkTYOD1bcJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Digging into Image Encoder\n",
        "\n",
        "Modern VLMs (such as [Gemma-3](https://deepmind.google/models/gemma/gemma-3/), [PaliGemma](https://arxiv.org/abs/2407.07726) for example) use pretrained ImageEncoders via [CLIP](https://arxiv.org/abs/2103.00020) loss (see below).\n",
        "\n",
        "Here, for example, is the architecture of the [PaliGemma](https://arxiv.org/abs/2407.07726) VLM, it uses [SigLIP](https://arxiv.org/abs/2303.15343) vision encoder which is a variant Vision Transformer (ViT). It also uses **Early Fusion** mechanism\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/paligemma/paligemma_arch.png\"\n",
        "alt=\"drawing\" width=\"600\"/>"
      ],
      "metadata": {
        "id": "qGMCLvta1fM8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importance of pre-trained image encoder üß†\n",
        "\n",
        "The performance of a Vision-Language Model is heavily dependent on **the quality of its pre-trained image encoder**. This is clearly demonstrated in the [PaliGemma](https://arxiv.org/pdf/2407.07726) paper, which reported a significant performance drop when training an image encoder from scratch compared to using a pre-trained one.\n",
        "\n",
        "The graph below visualizes this comparison:\n",
        "\n",
        "**üîµ Pre-trained (Blue)**: The image encoder is a powerful [SigLIP](https://arxiv.org/abs/2303.15343) model, pre-trained on a large-scale image-text dataset.\n",
        "\n",
        "**üü† From Scratch (Orange)**: The image encoder learns from raw image patches only during the VLM's training.\n",
        "\n",
        "As the results show, visual pre-training is crucial for achieving state-of-the-art performance and sample efficiency.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1fedemG4EszwhXqUDk1KyAKuwikC5a8ec\">"
      ],
      "metadata": {
        "id": "EG4K1dxo1lPX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vision transformer (ViT) Architecture\n",
        "\n",
        "ViT is the main architecture used for image encoders in VLMs.\n",
        "\n",
        "Let's look at the **[Vision Transformer](https://arxiv.org/abs/2010.11929) (ViT)** architecture, illustrated in the diagram. The core idea of ViT is to adapt the successful [Transformer](https://arxiv.org/abs/1706.03762) model, originally from NLP, to process images. This is achieved through a simple, effective process:\n",
        "\n",
        "* **Image Patchification & Embedding**: The input image is first deconstructed into a sequence of fixed-size, non-overlapping patches. You can think of these patches as the visual equivalent of \"words\" üñºÔ∏è. Each patch is then flattened and linearly projected into a vector. This creates a sequence of \"image tokens\" that the Transformer can understand.\n",
        "\n",
        "* **Transformer Encoder**: Finally, this sequence of tokens is fed into a standard Transformer Encoder, which processes the relationships between the patches to understand the image as a whole.\n",
        "\n",
        "* **(Optional) MLP Head**: For a downstream task like image classification, the output representation from the Transformer is passed to a final MLP (Multi-Layer Perceptron) head, which produces the final prediction (e.g., the object class). üéØ\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1UxoNoTXy39aIL8RPnpMVbX7wzmXCKlq6\">\n",
        "\n"
      ],
      "metadata": {
        "id": "_9p1CLJLzeC_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Contrastive Language-Image Pre-training (CLIP)\n",
        "\n",
        "The main method to pretrain image encoders!"
      ],
      "metadata": {
        "id": "uxoiT--yj-KE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Contrastive Language-Image Pre-training (CLIP)](https://arxiv.org/pdf/2103.00020) is a method that uses large-scale image-text datasets to learn a shared embedding space. In this space, the representations of images and their corresponding text descriptions are close together, while unrelated pairs are pushed far apart.\n",
        "\n",
        "The CLIP model architecture consists of **two encoders**: a **text encoder** and an **image encoder**. These encoders are used to generate representations for text and images, respectively. During training, the model's objective is to predict which image-text pairs within a batch are correctly matched. It achieves this by maximizing the similarity between the embeddings of positive (matching) pairs and minimizing the similarity of negative (mismatching) pairs.\n",
        "\n",
        "CLIP's training approach enables it to perform remarkably well on various image-related tasks, particularly in zero-shot settings where it can classify new images without needing to be fine-tuned on a specific dataset.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1bxFxZX7Amwdn4JCyrgZBy87fQDI1yCWM\" height=\"300\" width=\"500\">"
      ],
      "metadata": {
        "id": "oy8Wcher2RRP"
      }
    }
  ]
}