{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "machine_shape": "hm",
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/agalashov/m2ls_vlm_tutorial_private/blob/main/vlm_tutorial_practical_3_students.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QuolPkoI7i9X"
      },
      "source": [
        "# M2LS 2025: Vision-Language Models -- Practical 3\n",
        "---\n",
        "- Alexandre Galashov (agalashov@google.com)\n",
        "- Petra Bevandic (Petra.Bevandic@fer.hr)\n",
        "<br>\n",
        "[link to colab]\n",
        "\n",
        "In this practical session, you'll learn how to use and adapt **Vision-Language Models (VLMs)** for real-world tasks without having to train them from scratch.\n",
        "\n",
        "Large and powerful VLMs, such as [Gemma-3-12B-IT](https://huggingface.co/google/gemma-3-12b-it), will often perform well on a wide variety of real-world tasks straight out of the box. Smaller and **more efficient models** may be also used for specific use cases, but they will often require additional fine-tuning. This is not a major limitation, as relatively inexpensive and efficent methods for finetuning are being developed to complement the progress of large foundational models. In this particular practical, we will use [**Low-Rank Adaptation (LoRA)**](https://arxiv.org/abs/2106.09685).\n",
        "\n",
        "**In this tutorial, you will:**\n",
        "\n",
        "1. Evaluate a pre-trained VLM by testing its out-of-the-box (zero-shot) performance on a task of generating Amazon product descriptions\n",
        "\n",
        "2. Fine-tune a smaller VLM using LoRA to significantly boost its effectiveness.\n",
        "\n",
        "**Disclaimer**: You will mainly be required to complete code blocks which we noted as **\"Your code here\"**. We took care of most of the boilerplate code for you. However, please also feel free to deviate from the code which we prepared and code things in the way you feel is right!\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Compute requirements\n",
        "\n",
        "This notebook will require use a GPU with large memory in order to be able to train a model. If you do not have access to a GPU with large memory, we also pretrained a model for you. It is available as `\"agalashov/vlm-tutorial-finetuned-llm-final\"` from HuggingFace."
      ],
      "metadata": {
        "id": "S5odjjQlZUh9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Instal dependencies"
      ],
      "metadata": {
        "id": "_BCb3RTucoBu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install  -U -q transformers trl datasets bitsandbytes peft accelerate"
      ],
      "metadata": {
        "id": "UJu7ZLf5csk0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preliminary Setup (Hugging Face account)\n",
        "---\n",
        "\n",
        "1. Make a HuggingFace account if you already don't have one (Sign Up).\n",
        "\n",
        "2. Create (if  you have not done so already) an access token in HuggingFace.\n",
        "\n",
        "3. Either specify `HF_TOKEN` secret in colab secrets or specify `MANUALLY_ENTERED_HF_TOKEN`."
      ],
      "metadata": {
        "id": "AQf2oTeTXvaG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "from huggingface_hub import login\n",
        "\n",
        "MANUALLY_ENTERED_HF_TOKEN = '' # If not specified `HF_TOKEN`, enter your token.\n",
        "\n",
        "try:\n",
        "  HF_TOKEN = userdata.get('HF_TOKEN')\n",
        "except userdata.SecretNotFoundError:\n",
        "  HF_TOKEN = MANUALLY_ENTERED_HF_TOKEN\n",
        "\n",
        "login(token=HF_TOKEN)"
      ],
      "metadata": {
        "id": "e4zSADrjYZAK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 1 - Zero-shot generation of Amazon product descriptions\n",
        "\n",
        "In this exercise, we will see how we can use VLMs to automatically generate Amazon product descriptions given their images.\n",
        "\n",
        "To do this, we will:\n",
        "\n",
        "1. Load a pretrained VLM and inspect its components;\n",
        "2. Load the dataset of Amazon product descriptions and inspect the data;\n",
        "3. Create a prompt from the data to be able to use for inference in the pretrained VLM;\n",
        "4. Inspect the performance of this pretrained VLM on the product descriptions.\n",
        "\n",
        "We start by importing the required modules."
      ],
      "metadata": {
        "id": "zDoITdI0mbiW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Run imports\n",
        "from datasets import load_dataset\n",
        "from PIL import Image\n",
        "import torch\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {DEVICE}\")"
      ],
      "metadata": {
        "id": "yzNh6zU5dLOu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Prep task 1**: Load the model\n",
        "\n",
        "We will load one of two models depending on the value of `SMOL_MODEL`, either [`SmolVLM-256M-Base`](https://huggingface.co/HuggingFaceTB/SmolVLM-256M-Base) (when `SMOL_MODEL=True`) or [`Gemma-3-4B`](https://huggingface.co/google/gemma-3-4b-pt) (when `SMOL_MODEL=False`).\n",
        "\n",
        "[`SmolVLM-256M-Base`](https://huggingface.co/HuggingFaceTB/SmolVLM-256M-Base) is one of the smallest VLMs available in hugging face. This makes it a good choice when getting acquainted with the finetuning task as it requires significantly less computational resources, especially when compared to [`Gemma-3-4B`](https://huggingface.co/google/gemma-3-4b-pt) (or any other bigger model). Keep in mind that its small size will hinder its performance in real-world applications."
      ],
      "metadata": {
        "id": "f4f9fGwjdgkq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SMOL_MODEL = True # Choose whether to use a small model or not"
      ],
      "metadata": {
        "id": "ioyxoJu0dnF6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Idefics3ForConditionalGeneration, AutoProcessor\n",
        "from transformers import AutoProcessor, AutoModelForImageTextToText, BitsAndBytesConfig\n",
        "\n",
        "if SMOL_MODEL:\n",
        "  model_id = \"HuggingFaceTB/SmolVLM-256M-Base\"\n",
        "  chat_template_model_id = \"HuggingFaceTB/SmolVLM-256M-Instruct\"\n",
        "else:\n",
        "  model_id = \"google/gemma-3-4b-pt\"\n",
        "  chat_template_model_id = \"google/gemma-3-4b-it\"\n",
        "\n",
        "processor = AutoProcessor.from_pretrained(chat_template_model_id)\n",
        "model = AutoModelForImageTextToText.from_pretrained(\n",
        "      model_id,\n",
        "      device_map=\"auto\",\n",
        "      torch_dtype=torch.bfloat16,\n",
        "      # In principle, you could use flash attention to speed up performance.\n",
        "      _attn_implementation=\"eager\",\n",
        "  ).to(DEVICE)"
      ],
      "metadata": {
        "id": "DLpg6FsNdiPX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Have a look at the architecture of the model."
      ],
      "metadata": {
        "id": "QVDmcgFLAz1o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "id": "83uvCHvLAx4U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you are using `SmolVLM`, the underlying model class is `Idefics3Model`, the description is provided here:\n",
        "\n",
        "https://huggingface.co/docs/transformers/v4.53.3/en/model_doc/idefics3#transformers.Idefics3Model\n",
        "\n",
        "The corresponding code is provided here:\n",
        "\n",
        "https://github.com/huggingface/transformers/blob/v4.53.3/src/transformers/models/idefics3/modeling_idefics3.py#L601\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "vVGwSlObDV3D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question**: How is visial and text input processed via the model? You can study the code (here)[https://github.com/huggingface/transformers/blob/v4.53.3/src/transformers/models/idefics3/modeling_idefics3.py#L601]\n",
        "\n"
      ],
      "metadata": {
        "id": "vh24vYjwEBic"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer**: To fill"
      ],
      "metadata": {
        "id": "dqbYMNMD2dhk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('Do not forget to answer! :)')"
      ],
      "metadata": {
        "id": "liADi9K-2e3g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Prep Task 2**: Load and inspect dataset\n",
        "\n",
        "We load the dataset of Amazon product descriptions. Please inspect the entries of the dataset."
      ],
      "metadata": {
        "id": "8U1TaPuzc2Y4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset from the hub\n",
        "dataset = load_dataset(\"philschmid/amazon-product-descriptions-vlm\", split=\"train\")"
      ],
      "metadata": {
        "id": "TFkTuuCHdIGP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inspect the first element of the dataset and the information it contains."
      ],
      "metadata": {
        "id": "HhjUlrpi65Ol"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = dataset[0]\n",
        "data"
      ],
      "metadata": {
        "id": "Rf0MVKAz_lMk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The image of the product is stored in the `image` value."
      ],
      "metadata": {
        "id": "kczkKNBq_vjj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data['image']"
      ],
      "metadata": {
        "id": "DrBq_HGEdNr8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Corresponding product description is given by `description` entry"
      ],
      "metadata": {
        "id": "25HsQayQ_ziu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data['description']"
      ],
      "metadata": {
        "id": "5YczYOnf_3nW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The data entry contains some additional meta-data which can be useful"
      ],
      "metadata": {
        "id": "UMIJzMqEAAWJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(data[\"Product Name\"])\n",
        "print(data[\"Category\"])\n",
        "# ...."
      ],
      "metadata": {
        "id": "rO4AhY0uAKOe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Zero shot generation of product descriptions\n",
        "\n",
        "For this part of a practical, we will use a pre-trained VLM to automatically generate product descriptions in a structured format.\n",
        "\n",
        "We will be doing this in a **zero-shot setting**. This means we will prompt the model to perform the task directly, relying entirely on its pre-trained capabilities without any additional training or fine-tuning."
      ],
      "metadata": {
        "id": "9CDJKhLRmdn5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Visual Question Answering (VQA) task"
      ],
      "metadata": {
        "id": "LDtHNKZlCv-k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The VLMs are trained to solve **Visual Question Answering (VQA) task**, meaning that they are supposed to provide a text **answer** given a **text question** and **an image** (or multiple images)\n",
        "\n",
        "When querying VLM, we provide `query_prompt` which contains `question` and special image tokens (often, `<image>`). The image input is provided separately, so that the corresponding image embedding of fixed size will replace `<image>` token by a fixed size embedding.\n",
        "\n",
        "Here is an example of a `query_prompt`:\n",
        "\n",
        "`query_prompt=\"What do you see?<image>\"`\n",
        "\n",
        "A pseudocode of a VLM query:\n",
        "\n",
        "`answer=vlm(query_prompt=query_prompt,image=image)`\n",
        "\n",
        "When calling `vlm`, the `query_prompt` is processed as follows:\n",
        "\n",
        "* We embed the question `What do you see?` to a `[text_embedding]` of certain size. For example, it can have a shape `<num_text_tokens,embed_dim>`, where `num_text_tokens` is the number of text tokens in the question and `embed_dim` is some fixed embedding dimension.\n",
        "\n",
        "* We embed the image `image` to a `[image_embedding]` of fixed size `<fixed_num_image_tokens,embed_dim>`, where `fixed_num_image_tokens` is a hardcoded number of image tokens (often, 256) and `embed_dim` is the same embedding dimension as for text.\n",
        "\n",
        "* Both embeddings are combined to `query_embedding=[text_embedding][image_embedding]` a vector of shape `<num_text_tokens+fixed_num_image_tokens, embed_dim>`\n",
        "\n",
        "* The `query_embedding` is then processed by the corresponding transformer (LLM)."
      ],
      "metadata": {
        "id": "_oFSHWtpAOuO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chat format\n",
        "\n",
        "\n",
        "VLM promts should follow consistent format which is often VLM specific. This means that we will have to carefully design both the `query_prompt` and `answer` to match the expected structure.\n",
        "\n",
        "A widely used option is the chat template provided by the [`processor.apply_chat_template`](https://huggingface.co/docs/transformers/en/chat_templating) function. Since many LLMs and VLMs are trained with conversational data, they tend to perform better when prompts and responses are expressed in this format as well\n",
        "\n",
        "When constructing a prompt for VLM, we tipically provide the following entries:\n",
        "\n",
        "* **System prompt**: A short instruction to guide the overall VLM behaviour. Typically fixed for all different **User prompts**.\n",
        "\n",
        "* **User prompt**: The actual input which contains an optional task description, a question and the associated image.\n",
        "\n",
        "* **Assistant prompt**: The expected response that needs to be generated by the model (needed only for training/fine-tuning)."
      ],
      "metadata": {
        "id": "pygCjBuKiXOh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The **chat format** for a system identifying animals in images would look something like this:\n",
        "\n",
        "```\n",
        "messages = [\n",
        "  {\n",
        "    \"role\": \"system\", \"content\": [\n",
        "      {\"type\": \"text\", \"text\": \"You are friendly AI.\"}\n",
        "    ]\n",
        "  },\n",
        "  {\n",
        "    \"role\": \"user\", \"content\": [\n",
        "      {\"type\": \"text\", \"text\": \"What do you see on the picture?\"},\n",
        "      {\"type\": \"image\", \"image\": \"<image>\"}\n",
        "    ]\n",
        "  },\n",
        "  # You do not add this in the inference phase\n",
        "  {\n",
        "    \"role\": \"assis\", \"assistant\": [\n",
        "      {\"type\": \"text\", \"text\": \"This is a picture of a cat.\"}\n",
        "    ]\n",
        "  },\n",
        "]\n"
      ],
      "metadata": {
        "id": "XSgm4Wbdj2FQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Task 1**: Create a `system_prompt` and a `user_prompt`\n",
        "\n",
        "Come up with a potential `system_prompt` and `user_prompt` for product descriptions. You can provide any information from `data` into both prompts, as you want. Just do not provide `description`, since it is the answer. :)"
      ],
      "metadata": {
        "id": "Aa94NWTON5fI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "################################################################################\n",
        "# YOUR CODE HERE\n",
        "################################################################################\n",
        "...\n",
        "system_prompt = \"\" # TO FILL\n",
        "user_prompt = \"\" # TO FILL"
      ],
      "metadata": {
        "id": "2QXyS4hJjk9N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Task 2**: Create chat-style prompt.\n",
        "\n",
        "Write a function that generates a chat-style prompt for product description (see above). The function should follow the chat format.\n",
        "\n",
        "Since this function will also be reused in the fine-tuning exercise, include an option to control whether an assistant (answer) message is added (e.g., through a parameter like `add_assistant_message`)."
      ],
      "metadata": {
        "id": "xqOohAQ0OE3a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def format_data(data, system_prompt, user_prompt, add_assistant_message=True):\n",
        "  messages = []\n",
        "  ##############################################################################\n",
        "  # YOUR CODE HERE\n",
        "  ##############################################################################\n",
        "  ...\n",
        "  return messages"
      ],
      "metadata": {
        "id": "vQgK2mKzh282"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example = {'Product Name': 'Hello', 'Category': 'Cheese.', 'description': 'This is cheesecake.'}\n",
        "formatted_data = format_data(example, system_prompt, user_prompt, add_assistant_message=True)"
      ],
      "metadata": {
        "id": "ryiaQvH8la9O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Take a look on `formatted_data` and confirm that it is consistent with the chat template."
      ],
      "metadata": {
        "id": "0H-bwcRUlvjT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "formatted_data"
      ],
      "metadata": {
        "id": "bE_yk6U9lxnW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see what we get as a result of `apply_chat_template`"
      ],
      "metadata": {
        "id": "xmwbhqaalzWe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = processor.apply_chat_template(formatted_data, add_generation_prompt=False, tokenize=False)\n",
        "text"
      ],
      "metadata": {
        "id": "uJOlMfL7lrPj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that we use `add_generation_prompt=False`. If you set it to `add_generation_prompt=True`, it will add an additional `\"Assistant: \"` string to the end of the prompt, encouraging the VLM to respond.\n",
        "\n",
        "See how it behaves"
      ],
      "metadata": {
        "id": "zgcdICBRIo3v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = processor.apply_chat_template(formatted_data, add_generation_prompt=True, tokenize=False)\n",
        "text"
      ],
      "metadata": {
        "id": "kL6PWnrvI0S8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This however is not ideal, because we now inserted two `\"Assistant: \"` strings to the prompts.\n",
        "\n",
        "Here is how we fix it"
      ],
      "metadata": {
        "id": "0plm8wzPI5Gf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "example = {'Product Name': 'Hello', 'Category': 'Cheese.', 'description': 'This is cheesecake.'}\n",
        "formatted_data = format_data(example, system_prompt, user_prompt, add_assistant_message=False)\n",
        "text = processor.apply_chat_template(formatted_data, add_generation_prompt=True, tokenize=False)\n",
        "text"
      ],
      "metadata": {
        "id": "SMJYfnBYJAIE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ultimately, the prompt which ends by `\"Assistant: \"` is the one which we want to use for **inference**, while the the prompt which contains already a pre-defined answer `\"Assistant: some answer\"` should be used for **training**."
      ],
      "metadata": {
        "id": "_1mMFGQfJF1H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Important**: Make sure that \"\\<image\\>\" is part of the text. If it is not, you must modify `format_data` accordingly"
      ],
      "metadata": {
        "id": "yPxsgiOS35Nb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if \"<image>\" not in text:\n",
        "  raise ValueError(\"Tag <image> must be in the text!\")"
      ],
      "metadata": {
        "id": "YByVOb6x3wMp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For future, it will be useful to have a function which takes a data entry, formats it and applies the chat template."
      ],
      "metadata": {
        "id": "4yq3jYNpEsYJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_text_for_vlm(data, system_prompt, user_prompt, use_for_training):\n",
        "  text = \"\"\n",
        "  ##############################################################################\n",
        "  # YOUR CODE HERE\n",
        "  ##############################################################################\n",
        "  ...\n",
        "\n",
        "  return text"
      ],
      "metadata": {
        "id": "xPGNvwIqEp6t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Confirm it works in a way you expect"
      ],
      "metadata": {
        "id": "Fb-tRqLNE7xc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result = prepare_text_for_vlm(example, system_prompt, user_prompt, use_for_training=True)\n",
        "assert len(result.split('Assistant:')[1]) > 0\n",
        "assert result.endswith(\"<end_of_utterance>\\n\")\n",
        "assert \"<image>\" in result\n",
        "print('`prepare_text_for_training` is correctly implemented')\n",
        "print()\n",
        "result"
      ],
      "metadata": {
        "id": "PlzlckLRE3Rs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = prepare_text_for_vlm(example, system_prompt, user_prompt, use_for_training=False)\n",
        "assert result.endswith(\"Assistant:\")\n",
        "assert \"<image>\" in result\n",
        "print('`prepare_text_for_inference` is correctly implemented')\n",
        "print()\n",
        "result"
      ],
      "metadata": {
        "id": "2fFa1GUMJlWX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we are ready to query VLM for providing product descriptions"
      ],
      "metadata": {
        "id": "h5OPTqtQJrTy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Task 3**: Run the VLM to get zero-shot product descriptions.\n",
        "\n",
        "Now, we will query the VLM to generate the product descriptions. First, we select any example from a dataset."
      ],
      "metadata": {
        "id": "sm5-Gs9EsDbE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check a dataset example\n",
        "example_idx = 2 # Feel free to modify\n",
        "data = dataset[example_idx]\n",
        "print(f\"Product name: {data['Product Name']}\")\n",
        "print(f\"Category: {data['Category']}\")\n",
        "print(f\"Description: {data['description']}\")\n",
        "data['image'].convert('RGB')"
      ],
      "metadata": {
        "id": "IlyoWrUUnE_6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need to produce `inputs` to the model as follows."
      ],
      "metadata": {
        "id": "9vBH38D4sN0O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query_prompt = prepare_text_for_vlm(data, system_prompt, user_prompt, use_for_training=False)\n",
        "image = data['image'].convert('RGB')\n",
        "# Preprocessing inputs\n",
        "inputs = processor(text=[query_prompt], images=[image], return_tensors=\"pt\", padding=True).to(DEVICE)"
      ],
      "metadata": {
        "id": "ZOhhB_SunUmZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inspect `inputs`"
      ],
      "metadata": {
        "id": "zEDd8kwns-5a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs"
      ],
      "metadata": {
        "id": "gTQC3Oq6tAaw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Take a look on what keys are produced"
      ],
      "metadata": {
        "id": "vwC4NzBJGZVN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for k in inputs.keys():\n",
        "  print(k)"
      ],
      "metadata": {
        "id": "vmwAuqDcGXNs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we will implement a generating (inference) function.\n",
        "\n",
        "We will use `model.generate` function to achieve that. Have a look on the signature of this function"
      ],
      "metadata": {
        "id": "eRX5Txv-GeXv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.generate"
      ],
      "metadata": {
        "id": "JK2btMIeGkCk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will implement `generate_product_description` which receives `query_prompt`, `image` as well as `max_new_tokens` (which controls the amount of output tokens). It is useful to specify this parameter to decrease the inference time.\n",
        "\n",
        "It then should call `model.generate` to generate product descriptions."
      ],
      "metadata": {
        "id": "0Ak1rxO2Gy00"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_product_description(query_prompt, image, max_new_tokens = 256):\n",
        "  # Preprocessing inputs\n",
        "  inputs = processor(text=[query_prompt], images=[image], return_tensors=\"pt\", padding=True).to(DEVICE)\n",
        "  ##############################################################################\n",
        "  # YOUR CODE HERE\n",
        "  ##############################################################################\n",
        "  generated_ids = ...\n",
        "\n",
        "  # `generated_ids` contains both, input ids (from the prompt) as well as the\n",
        "  # generated ones. Thus, we need to \"trim\" it only to keep the output part.\n",
        "  generated_ids_trimmed = [out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)]\n",
        "  output_text = processor.batch_decode(\n",
        "      generated_ids_trimmed,\n",
        "      skip_special_tokens=True,\n",
        "      clean_up_tokenization_spaces=False)\n",
        "\n",
        "  return output_text[0]"
      ],
      "metadata": {
        "id": "dIK9BEJDGrwk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's generate the descriptions"
      ],
      "metadata": {
        "id": "TMc-BSMdHTSZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output_text = generate_product_description(query_prompt, image, max_new_tokens=32)\n",
        "output_text"
      ],
      "metadata": {
        "id": "t9FKE6fvHHlw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we see, the model outputs a bit of a nonsense.\n",
        "\n",
        "Thus, we need to finetune it."
      ],
      "metadata": {
        "id": "u9lbkNndtVJy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's clean up the memory in order to unload the model."
      ],
      "metadata": {
        "id": "vCNvjryiQAC3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "del model\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "gyPPZhHrP_oQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 2 - Finetuning the VLM to improve product description generation\n",
        "\n",
        "In our case, the zero-shot performance of the model for generating product descriptions is quite poor. This is due to the fact that the model was not instruction tuned (i.e. it does not know how to follow the user instructions in general) and is not adapted for the specialized task of product description.\n",
        "\n",
        "In order to adapt the model for a task in hand, we should finetune it.\n",
        "\n",
        "For that, we are going to use [LoRA](https://arxiv.org/abs/2106.09685) (Low Rank Adaptation) with quantization, which is called [QLoRA](https://arxiv.org/abs/2305.14314), which enables cheap and memory-efficient fine-tuning.\n",
        "\n",
        "To achieve this, we will be using [SFTConfig](https://huggingface.co/docs/trl/sft_trainer) class from the [TRL library](https://huggingface.co/docs/trl/index)."
      ],
      "metadata": {
        "id": "FvSdM-Ryte82"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Pretrained model checkpoint\n",
        "\n",
        "Note that during the tutorial it might be challenging to actually run the finetuning because you can run out of memory. We have prepared a finetuned model checkpoint for you (available only when `SMOL_MODEL=True`). You can use it by setting `USE_PRETRAINED_MODEL_CHECKPOINT=True`."
      ],
      "metadata": {
        "id": "macTQS_XJdTi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PRETRAINED_MODEL_CHECKPOINT = \"agalashov/vlm-tutorial-finetuned-llm-final\"\n",
        "\n",
        "USE_PRETRAINED_MODEL_CHECKPOINT = False\n",
        "\n",
        "if USE_PRETRAINED_MODEL_CHECKPOINT:\n",
        "  if not SMOL_MODEL:\n",
        "    raise ValueError('Must use SMOL_MODEL = True.')"
      ],
      "metadata": {
        "id": "wOPRGDvnJclw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load the int-4 quantized model\n",
        "\n",
        "We will be using `BitsAndBytesConfig` package which allows us to load the model in the quantized way in order to reduce the memory. This is what we are going to use for QLoRA.\n",
        "\n"
      ],
      "metadata": {
        "id": "GNJpLxmkNYZb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BitsAndBytesConfig\n",
        "\n",
        "# BitsAndBytesConfig int-4 config\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    # `load_in_4bit=True`: This is the main argument that enables the 4-bit\n",
        "    # quantization. When set to `True`, the model's weights will be loaded\n",
        "    # and stored in 4-bit format, drastically reducing memory consumption.\n",
        "    load_in_4bit=True,\n",
        "    # `bnb_4bit_use_double_quant=True`: This enables \"double quantization.\"\n",
        "    # It quantizes the quantization constants themselves, which are the values\n",
        "    # used to de-quantize the 4-bit weights back to their original form. This\n",
        "    # can save an additional 0.4 bits per parameter, leading to a further\n",
        "    # small reduction in memory.\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    # `bnb_4bit_quant_type=\"nf4\"`: This specifies the type of 4-bit\n",
        "    # quantization to use. \"nf4\" stands for \"NormalFloat 4-bit.\" It is a\n",
        "    # quantization data type that is theoretically optimal for weights that\n",
        "    # follow a normal distribution, which is common in transformer models.\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    # `bnb_4bit_compute_dtype=torch.bfloat16`: This sets the data type for\n",
        "    # computations that are performed during the forward and backward passes.\n",
        "    # Even though the weights are stored in 4-bit, the actual matrix\n",
        "    # multiplications need to be done in a higher precision. `bfloat16`\n",
        "    # (Brain Floating Point 16-bit) is a good choice as it provides a\n",
        "    # larger dynamic range than `float16`, which is beneficial for training.\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "# Load model and tokenizer\n",
        "model = AutoModelForImageTextToText.from_pretrained(\n",
        "      model_id,\n",
        "      device_map=\"auto\",\n",
        "      torch_dtype=torch.bfloat16,\n",
        "      quantization_config=bnb_config,\n",
        "      _attn_implementation=\"eager\",\n",
        "  ).to(DEVICE)\n",
        "processor = AutoProcessor.from_pretrained(chat_template_model_id)"
      ],
      "metadata": {
        "id": "vupBIuiauLkz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65wfO29isQlX"
      },
      "source": [
        "### **Task 1**: Set Up QLoRA\n",
        "\n",
        "Next, we will configure [QLoRA](https://github.com/artidoro/qlora) for our training setup. QLoRA allows efficient fine-tuning of large models by reducing the memory footprint. Unlike traditional LoRA, which uses low-rank approximation, QLoRA further quantizes the LoRA adapter weights, leading to even lower memory usage and faster training.\n",
        "\n",
        "We took care of most of a boiler plate, just fill the arguments for `LoraConfig` !"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ITmkRHWCKYjf"
      },
      "outputs": [],
      "source": [
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "lora_rank = 8\n",
        "\n",
        "# Configure LoRA\n",
        "peft_config = LoraConfig(\n",
        "    # `r`: This is the LoRA rank. It determines the size of the low-rank\n",
        "    # matrices that are added to the model. A higher rank means more trainable\n",
        "    # parameters and potentially better performance, but also more memory\n",
        "    # usage.\n",
        "\n",
        "    # YOUR CODE HERE\n",
        "    r=TO_FILL,\n",
        "\n",
        "    # `lora_alpha`: This is the scaling factor for the LoRA update. It\n",
        "    # controls the magnitude of the LoRA matrices. The scaling is often\n",
        "    # set to `lora_alpha / r` by default, so in this case, the scaling\n",
        "    # factor would be 1. A higher alpha can make the updates more\n",
        "    # significant.\n",
        "\n",
        "    # YOUR CODE HERE\n",
        "    lora_alpha=TO_FILL,\n",
        "\n",
        "    # `lora_dropout`: This is the dropout probability for the LoRA\n",
        "    # layers. It helps prevent overfitting during fine-tuning by randomly\n",
        "    # setting some of the LoRA weights to zero.\n",
        "\n",
        "    # YOUR CODE HERE\n",
        "    lora_dropout=TO_FILL,\n",
        "\n",
        "\n",
        "    # `target_modules`: This specifies the names of the model's layers\n",
        "    # (or modules) that the LoRA adapter will be applied to. It's common\n",
        "    # to target the attention layers' linear projections, such as `q_proj`\n",
        "    # (query), `k_proj` (key), `v_proj` (value), and `o_proj` (output).\n",
        "    # Targeting the MLP layers (`gate_proj`, `up_proj`, `down_proj`) is\n",
        "    # also a common practice.\n",
        "\n",
        "    # YOUR CODE HERE -- You can investigate `model` structure to see which\n",
        "    # parameters to target.\n",
        "    target_modules=TO_FILL\n",
        "\n",
        "    # `use_dora`: This enables \"DoRA\" (Weight-Decomposed LoRA), an\n",
        "    # improvement on standard LoRA. DoRA works by decomposing the LoRA\n",
        "    # weights into two components, which can lead to better fine-tuning\n",
        "    # performance, especially on smaller models.\n",
        "\n",
        "    # YOUR CODE HERE\n",
        "    use_dora=TO_FILL,\n",
        "\n",
        "    # `init_lora_weights=\"gaussian\"`: This determines how the LoRA matrices\n",
        "    # are initialized. `\"gaussian\"` means the weights are drawn from a\n",
        "    # Gaussian (normal) distribution. Other options include `\"loftq\"`.\n",
        "    init_lora_weights=\"gaussian\"\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once you have configured LoRA, apply it to the model to get only `LoRA`-trainable model."
      ],
      "metadata": {
        "id": "wrEKgnPXKxLt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply PEFT model adaptation\n",
        "peft_model = get_peft_model(model, peft_config)\n",
        "\n",
        "# Print trainable parameters\n",
        "peft_model.print_trainable_parameters()"
      ],
      "metadata": {
        "id": "YKxQZnUEKbFs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice that the number of parameters we are going to train is significantly lower than the original model"
      ],
      "metadata": {
        "id": "Y6f0aASqK3lX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Task 2**: Set Up SFTConfig\n",
        "\n",
        "We will use Supervised Fine-Tuning (SFT) to improve our model's performance on the specific task. To achieve this, we'll define the training arguments with the [SFTConfig](https://huggingface.co/docs/trl/sft_trainer) class from the [TRL library](https://huggingface.co/docs/trl/index). SFT leverages labeled data to help the model generate more accurate responses, adapting it to the task. This approach enhances the model's ability to understand and respond to visual queries more effectively.\n",
        "\n",
        "Configure `SFTConfig`. You can read comments for the different hyperparameters. It is already preconfigured for you, but you can modify it in any way you want."
      ],
      "metadata": {
        "id": "QGPvGE_oLRA5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from trl import SFTConfig\n",
        "\n",
        "YOUR_OUTPUT_DIRECTORY = \"vlm-tutorial-finetuned-llm\"\n",
        "\n",
        "# Configure training arguments using SFTConfig\n",
        "args = SFTConfig(\n",
        "    output_dir=YOUR_OUTPUT_DIRECTORY,           # directory to save and repository id\n",
        "    num_train_epochs=2,                         # number of training epochs\n",
        "    per_device_train_batch_size=4,              # batch size per device during training\n",
        "    gradient_accumulation_steps=4,              # number of steps before performing a backward/update pass\n",
        "    gradient_checkpointing=True,                # use gradient checkpointing to save memory\n",
        "    optim=\"adamw_torch_fused\",                  # use fused adamw optimizer\n",
        "    logging_steps=5,                            # log every 5 steps\n",
        "    save_strategy=\"epoch\",                      # save checkpoint every epoch\n",
        "    learning_rate=2e-4,                         # learning rate, based on QLoRA paper\n",
        "    bf16=True,                                  # use bfloat16 precision\n",
        "    max_grad_norm=0.3,                          # max gradient norm based on QLoRA paper\n",
        "    warmup_ratio=0.03,                          # warmup ratio based on QLoRA paper\n",
        "    lr_scheduler_type=\"constant\",               # use constant learning rate scheduler\n",
        "    push_to_hub=True,                           # push model to hub\n",
        "    report_to=\"tensorboard\",                    # report metrics to tensorboard\n",
        "    gradient_checkpointing_kwargs={\n",
        "        \"use_reentrant\": False\n",
        "    },  # use reentrant checkpointing\n",
        "    dataset_text_field=\"\",                      # need a dummy field for collator\n",
        "    dataset_kwargs={\"skip_prepare_dataset\": True},  # important for collator\n",
        ")\n",
        "args.remove_unused_columns = False # important for collator"
      ],
      "metadata": {
        "id": "7n-rDmdrLOq5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### We need to set up `SFTTrainer`\n",
        "\n",
        "Now, we need to set up `SFTTrainer` -- a class, which will allow us to run an experiment.\n",
        "\n",
        "In order to set it up we need to define:\n",
        "\n",
        "* `model` -- already defined\n",
        "* `args` -- SFTConfig, already defined\n",
        "* `peft_config` -- peft_config, already defined\n",
        "* `processing_class` -- processor, already defined\n",
        "* `train_dataset` -- a dataset over which it will iterate. **NEED TO DEFINE**\n",
        "* `data_collator` -- a function which can parse every example from a dataset. **NEED TO DEFINE**"
      ],
      "metadata": {
        "id": "6Hfq5br6Luzx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Task 3**: Create a dataset of text-image data\n",
        "\n",
        "Set up a dataset `prompts_and_images` which will contain training prompts and images."
      ],
      "metadata": {
        "id": "e9GYmIwKMbsQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompts_and_images = []\n",
        "for data in dataset:\n",
        "  image = data['image'].convert('RGB')\n",
        "  ##############################################################################\n",
        "  # Your code here\n",
        "  ##############################################################################\n",
        "  ...\n",
        "\n"
      ],
      "metadata": {
        "id": "2xKNte-oMeRH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "assert len(prompts_and_images) == len(dataset)\n",
        "assert isinstance(prompts_and_images[0][0], str)\n",
        "assert isinstance(prompts_and_images[0][1], Image.Image)\n",
        "print('Looks like `prompts_and_images` is correctly implemented')"
      ],
      "metadata": {
        "id": "NScX4cO4Mtb6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompts_and_images[0][0]"
      ],
      "metadata": {
        "id": "nrWo5zJMRoBh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Task 4**: Implement the dataset processor\n",
        "\n",
        "We will implement `collate_fn(list_of_examples)`, which receives a list (a batch) of entries from `train_dataset` (in our case it is `prompts_and_images`).\n",
        "\n",
        "Note that we can call `processor(text=list_of_prompts, images=list_of_images, ...)` to process the list of prompts and images all together"
      ],
      "metadata": {
        "id": "PU4VySNCNJwO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a data collator to encode text and image pairs\n",
        "def collate_fn(examples):\n",
        "  texts, images = [], []\n",
        "  ##############################################################################\n",
        "  # YOUR CODE HERE\n",
        "  ##############################################################################\n",
        "  ...\n",
        "\n",
        "\n",
        "  batch = processor(text=texts, images=images, return_tensors=\"pt\", padding=True)\n",
        "  # We do an additional masking here.\n",
        "  # The labels are the input_ids, and we mask the padding tokens and image\n",
        "  # tokens in the loss computation\n",
        "  labels = batch[\"input_ids\"].clone()\n",
        "  # Mask image tokens\n",
        "  image_token_id = [processor.tokenizer.convert_tokens_to_ids(\"<image>\")]\n",
        "  # Mask tokens for not being used in the loss computation\n",
        "  labels[labels == processor.tokenizer.pad_token_id] = -100\n",
        "  labels[labels == image_token_id] = -100\n",
        "  labels[labels == 262144] = -100\n",
        "  batch[\"labels\"] = labels\n",
        "  return batch"
      ],
      "metadata": {
        "id": "LoHncqcENxcq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Create `SFTTrainer` instance"
      ],
      "metadata": {
        "id": "Y_pBxsKNOUMy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from trl import SFTTrainer\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=prompts_and_images,\n",
        "    peft_config=peft_config,\n",
        "    processing_class=processor,\n",
        "    data_collator=collate_fn,\n",
        ")"
      ],
      "metadata": {
        "id": "UOBQ88_aNPEm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Task 5**: Push the button on finetuning!"
      ],
      "metadata": {
        "id": "XjCUjxseOfNn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if not USE_PRETRAINED_MODEL_CHECKPOINT:\n",
        "  # Start training, the model will be automatically saved to the Hub and the output directory\n",
        "  trainer.train()\n",
        "\n",
        "  # Save the final model again to the Hugging Face Hub\n",
        "  trainer.save_model()"
      ],
      "metadata": {
        "id": "knqRAWtYPb2n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if not USE_PRETRAINED_MODEL_CHECKPOINT:\n",
        "  # need to free the memory\n",
        "  del model\n",
        "  del trainer\n",
        "  torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "HUAb8qdnPnNT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " `trainer.save_model()` saves only the LoRA parameters and not the full model. For convenience, you might want to save the full model."
      ],
      "metadata": {
        "id": "SNSLWh_oKvdh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if not USE_PRETRAINED_MODEL_CHECKPOINT:\n",
        "  from peft import PeftModel\n",
        "\n",
        "  # Load Model base model\n",
        "  model = AutoModelForImageTextToText.from_pretrained(model_id, low_cpu_mem_usage=True)\n",
        "\n",
        "  # Merge LoRA and base model and save\n",
        "  peft_model = PeftModel.from_pretrained(model, args.output_dir)\n",
        "  merged_model = peft_model.merge_and_unload()\n",
        "  merged_model.save_pretrained(\"merged_model\", safe_serialization=True, max_shard_size=\"2GB\")\n",
        "\n",
        "  processor = AutoProcessor.from_pretrained(args.output_dir)\n",
        "  processor.save_pretrained(\"merged_model\")"
      ],
      "metadata": {
        "id": "2x8uNUXqPsan"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Task 6**: Verify your finetuned model\n",
        "\n",
        "Now, we are going to query our finetuned model to see whether it provides more accurate product descriptions.\n",
        "\n",
        "First, we reload our newly trained model (or a pretrained checkpoint)."
      ],
      "metadata": {
        "id": "8pXguad2OjzI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "if USE_PRETRAINED_MODEL_CHECKPOINT:\n",
        "  model_dir = PRETRAINED_MODEL_CHECKPOINT\n",
        "else:\n",
        "  model_dir = args.output_dir\n",
        "\n",
        "# Load Model with PEFT adapter\n",
        "model = AutoModelForImageTextToText.from_pretrained(\n",
        "  model_dir,\n",
        "  device_map=\"auto\",\n",
        "  torch_dtype=torch.bfloat16,\n",
        "  _attn_implementation=\"eager\",\n",
        ")\n",
        "processor = AutoProcessor.from_pretrained(model_dir)"
      ],
      "metadata": {
        "id": "jKHIGftGiTX8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's query a specific example"
      ],
      "metadata": {
        "id": "xcCmbKhbLSrg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "example_idx = 2 # You can modify it\n",
        "data = dataset[example_idx]\n",
        "query_prompt = prepare_text_for_vlm(data, system_prompt, user_prompt, use_for_training=False)\n",
        "image = data['image'].convert('RGB')\n",
        "image"
      ],
      "metadata": {
        "id": "ND-oiIW3PORM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_text = generate_product_description(query_prompt, image, max_new_tokens=32)\n",
        "output_text"
      ],
      "metadata": {
        "id": "W8uaXkMCPWLD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The output here should make much more sense!"
      ],
      "metadata": {
        "id": "ycKoT772XOAS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using VLM for alternative real-world problems\n",
        "\n",
        "You now know how to fine-tune a VLM for a specific task, which primarily involves correct data formatting and scripting. As we've discussed, the standard VLM architecture solves tasks like **Visual Question Answering (VQA)** by combining text with images processed by a dedicated vision encoder.\n",
        "\n",
        "The key takeaway is that this is not limited to images. In fact, the image modality can be replaced by any other data type, such as:\n",
        "\n",
        "* Video clips\n",
        "\n",
        "* Audio recordings\n",
        "\n",
        "* Anything else\n",
        "\n",
        "The fundamental mechanism for combining the non-textual data with the text prompts can stay the same. By simply swapping the vision encoder for one suited to a different modality, you can adapt the VLM framework to a whole new range of problems."
      ],
      "metadata": {
        "id": "xpdG8HjLiOSp"
      }
    }
  ]
}